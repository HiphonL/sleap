
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>sleap.nn.architectures.common &#8212; SLEAP  documentation</title>
    <link rel="stylesheet" href="../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for sleap.nn.architectures.common</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Text</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Add</span>


<div class="viewcode-block" id="expand_to_n"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.expand_to_n">[docs]</a><span class="k">def</span> <span class="nf">expand_to_n</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Expands an object `x` to `n` elements if scalar.</span>

<span class="sd">    This is a utility function that wraps np.tile functionality.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Scalar of any type</span>
<span class="sd">        n: Number of repetitions</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tiled version of `x` with __len__ == `n`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Variable to expand must be scalar.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="scale_input"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.scale_input">[docs]</a><span class="k">def</span> <span class="nf">scale_input</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Rescale input to [-1, 1].&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span></div>


<div class="viewcode-block" id="tile_channels"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.tile_channels">[docs]</a><span class="k">def</span> <span class="nf">tile_channels</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tiles single channel to 3 channel.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span></div>


<div class="viewcode-block" id="conv"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.conv">[docs]</a><span class="k">def</span> <span class="nf">conv</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convenience presets for Conv2D.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_filters: Number of output filters (channels)</span>
<span class="sd">        kernel_size: Size of convolution kernel</span>
<span class="sd">        activation: Activation function applied to output</span>
<span class="sd">        **kwargs: Arbitrary keyword arguments passed on to keras.layers.Conv2D</span>

<span class="sd">    Returns:</span>
<span class="sd">        keras.layers.Conv2D instance built with presets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">num_filters</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="conv1"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.conv1">[docs]</a><span class="k">def</span> <span class="nf">conv1</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convenience presets for 1x1 Conv2D.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_filters: Number of output filters (channels)</span>
<span class="sd">        **kwargs: Arbitrary keyword arguments passed on to keras.layers.Conv2D</span>

<span class="sd">    Returns:</span>
<span class="sd">        keras.layers.Conv2D instance built with presets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">conv</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="conv3"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.conv3">[docs]</a><span class="k">def</span> <span class="nf">conv3</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convenience presets for 3x3 Conv2D.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_filters: Number of output filters (channels)</span>
<span class="sd">        **kwargs: Arbitrary keyword arguments passed on to keras.layers.Conv2D</span>

<span class="sd">    Returns:</span>
<span class="sd">        keras.layers.Conv2D instance built with presets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">conv</span><span class="p">(</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="residual_block"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.residual_block">[docs]</a><span class="k">def</span> <span class="nf">residual_block</span><span class="p">(</span><span class="n">x_in</span><span class="p">,</span> <span class="n">num_filters</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Residual bottleneck block.</span>

<span class="sd">    This function builds a residual block that is used at every step of stacked</span>
<span class="sd">    hourglass construction. Note that the layers are actually instantiated and</span>
<span class="sd">    connected.</span>

<span class="sd">    The bottleneck is constructed by applying a 1x1 conv with `num_filters / 2`</span>
<span class="sd">    channels, a 3x3 conv with `num_filters / 2` channels, and a 1x1 conv with</span>
<span class="sd">    `num_filters`. The output of this last conv is skip-connected with the input</span>
<span class="sd">    via an Add layer (the residual).</span>

<span class="sd">    If the input `x_in` has a different number of channels as `num_filters`, an</span>
<span class="sd">    additional 1x1 conv is applied to the input whose output will be used for the</span>
<span class="sd">    skip connection.</span>

<span class="sd">    Args:</span>
<span class="sd">        x_in: Input 4-D tf.Tensor or instantiated layer</span>
<span class="sd">        num_filters: The number output channels of the block. If not specified,</span>
<span class="sd">            defaults to the same number of channels as the input tensor. Must be</span>
<span class="sd">            divisible by 2 since the bottleneck halves the number of filters in</span>
<span class="sd">            the intermediate convs.</span>
<span class="sd">        batch_norm: Apply batch normalization after each convolution</span>

<span class="sd">    Returns:</span>
<span class="sd">        x_out: tf.Tensor of the output of the block of the same width and height</span>
<span class="sd">            as the input with `num_filters` channels.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Default to output the same number of channels as input</span>
    <span class="k">if</span> <span class="n">num_filters</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_filters</span> <span class="o">=</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Number of output channels must be divisible by 2</span>
    <span class="k">if</span> <span class="n">num_filters</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Number of output filters must be divisible by 2 in residual blocks.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># If number of input and output channels are different, add a 1x1 conv to use as the</span>
    <span class="c1"># identity tensor to which we add the residual at the end</span>
    <span class="n">x_identity</span> <span class="o">=</span> <span class="n">x_in</span>
    <span class="k">if</span> <span class="n">x_in</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">num_filters</span><span class="p">:</span>
        <span class="n">x_identity</span> <span class="o">=</span> <span class="n">conv1</span><span class="p">(</span><span class="n">num_filters</span><span class="p">)(</span><span class="n">x_in</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
            <span class="n">x_identity</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x_identity</span><span class="p">)</span>

    <span class="c1"># Bottleneck: 1x1 -&gt; 3x3 -&gt; 1x1 -&gt; Add residual to identity</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">conv1</span><span class="p">(</span><span class="n">num_filters</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)(</span><span class="n">x_in</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">conv3</span><span class="p">(</span><span class="n">num_filters</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">conv1</span><span class="p">(</span><span class="n">num_filters</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">batch_norm</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_out</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()([</span><span class="n">x_identity</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">x_out</span></div>


<span class="k">def</span> <span class="nf">upsampling_blocks</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">up_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">upsampling_layers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">interp</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">,</span>
    <span class="n">refine_conv_up</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">conv_filters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">up_blocks</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">upsampling_layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">UpSampling2D</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interp</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span>
                <span class="n">conv_filters</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_normal&quot;</span><span class="p">,</span>
            <span class="p">)(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">refine_conv_up</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
                <span class="n">conv_filters</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span>
            <span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="upsampled_average_block"><a class="viewcode-back" href="../../../../training.html#sleap.nn.architectures.common.upsampled_average_block">[docs]</a><span class="k">def</span> <span class="nf">upsampled_average_block</span><span class="p">(</span>
    <span class="n">tensors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">interp</span><span class="p">:</span> <span class="n">Text</span> <span class="o">=</span> <span class="s2">&quot;bilinear&quot;</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Upsamples tensors to a common size and reduce by averaging.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors: A list of tensors of possibly different heights/widths, but the same</span>
<span class="sd">            number of channels.</span>
<span class="sd">        target_size: Size that the tensors be upsampled to. If None, this is set to the</span>
<span class="sd">            size of the largest tensor in the list.</span>
<span class="sd">        interp: Interpolation method (&quot;nearest&quot; or &quot;bilinear&quot;).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A single tensor with the target size that is the average of all of the input</span>
<span class="sd">        tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">target_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Find biggest output.</span>
        <span class="n">largest_tensor</span> <span class="o">=</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">largest_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">largest_tensor</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">target_size</span> <span class="o">=</span> <span class="n">largest_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Now collect all outputs with upsampling if needed.</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">target_size</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">UpSampling2D</span><span class="p">(</span>
                <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">target_size</span> <span class="o">/</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">interp</span>
            <span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Now average everything.</span>
    <span class="n">averaged_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Average</span><span class="p">()(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">averaged_output</span></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">SLEAP</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorial-part2.html">Tutorial, Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../howtos.html">How-Tos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference.html">Feature Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api.html">API</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Murthy Lab @ Princeton.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>